# Inference Verification to Detect Model Weight Exfiltration.

This repo is associated with [Verifying LLM Inference to Detect Model Weight Exfiltration](https://arxiv.org/abs/2511.02620)

## Abstract
As large AI models become increasingly valuable assets, the risk of model weight exfiltration from inference servers grows accordingly.
An attacker controlling an inference server may exfiltrate model weights by hiding them within ordinary model responses, a strategy known as steganography.
This work investigates how to **verify** LLM model inference to defend against such attacks and, more broadly, to detect anomalous or buggy behavior during inference.
We formalize model weight exfiltration as a security game, propose a verification framework that can provably mitigate steganographic exfiltration, and specify the trust assumptions associated with our scheme. To enable verification, we characterize valid sources of non-determinism in large language model inference and introduce two practical estimators for them.
We evaluate our detection framework on several open-weight models ranging from 3B to 30B parameters. On MOE-Qwen-30B, our detector reduces exfiltratable information to <0.5% with false-positive rate of <0.01, corresponding to a >200x slowdown for adversaries.
Overall, this work further establishes a foundation for defending against model weight exfiltration and demonstrates that strong protection can be achieved with minimal additional cost to inference providers.

## Repo Description

This repository contains code for inference verification applied to preventing model weight exfiltration. Specifically we implement the Gumbel Likelihood Score (GLS) and Convolved Gaussian Score (CGS) methods for verifying tokens generated by large language models when running inference using the Inverse Probability Transform and Gumbel-Max Trick.

**Note on terminology:** In the paper, these methods are referred to as:
- **Inverse Probability Transform (IPT) Likelihood Score (IPT-LS)** - implemented in the codebase as **Convolved Gaussian Score (CGS)**
- **Gumbel Max Likelihood Score (GM-LS)** - implemented in the codebase as **Gumbel Likelihood Score (GLS)**


## Quick Start

Run the minimal test to verify the installation:

```bash
cd scripts
./test_minimal.sh
```

This will:
1. Generate text using vLLM (100 prompts, 100 tokens each)
2. Compute GLS/CGS verification scores
3. Produce Pareto frontier plots showing FPR vs extractable information trade-offs

Results are saved to `scripts/test_run_[timestamp]/`


### Multi-Model Sweep

For production experiments across multiple models:

```bash
cd scripts
./run_GLS_all_models.sh
```

## Code Contributors:
* Adam Karvonen
* Daniel Reuter
* Roy Rinberg


## Citation


```
@misc{rinberg2025verifyingllminferenceprevent,
      title={Verifying LLM Inference to Prevent Model Weight Exfiltration}, 
      author={Roy Rinberg and Adam Karvonen and Alex Hoover and Daniel Reuter and Keri Warr},
      year={2025},
      eprint={2511.02620},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2511.02620}, 
}
```
